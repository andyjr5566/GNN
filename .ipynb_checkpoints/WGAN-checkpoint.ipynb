{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n",
    "import keras\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_init = keras.initializers.RandomNormal(mean=0., stddev=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import keras.backend as K\n",
    "\n",
    "'''\n",
    "生成器（generator）\n",
    "首先，建立一個“生成器（generator）”模型，它將一個向量（從潛在空間 - 在訓練期間隨機取樣）轉換為候選影象。\n",
    "GAN通常出現的許多問題之一是generator卡在生成的影象上，看起來像噪聲。一種可能的解決方案是在鑑別器（discriminator）\n",
    "和生成器（generator）上使用dropout。\n",
    "'''\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 46\n",
    "height = 92\n",
    "width = 68\n",
    "channels = 3\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "\n",
    "x = layers.Dense(128 * 46 * 34)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Reshape((46, 34, 128))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, 4, strides=2, padding='same', kernel_initializer=weight_init)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same', kernel_initializer=weight_init)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# 生成一個 32x32 1-channel 的feature map\n",
    "x = layers.Conv2D(channels, 16, activation='tanh', padding='same', kernel_initializer=weight_init)(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "'''\n",
    "discriminator(鑑別器)\n",
    "建立鑑別器模型，它將候選影象（真實的或合成的）作為輸入，並將其分為兩類：“生成的影象”或“來自訓練集的真實影象”。\n",
    "'''\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3, kernel_initializer=weight_init)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(64, 4, strides=2, kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "x = layers.Conv2D(32, 4, strides=2, kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(32)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "\n",
    "x = layers.Conv2D(32, 4, strides=2, kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# 重要的技巧（新增一個dropout層）\n",
    "x = layers.Dropout(0,4)(x)\n",
    "\n",
    "# 分類層\n",
    "x = layers.Dense(1)(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "# 為了訓練穩定，在優化器中使用學習率衰減和梯度限幅（按值）。\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0005)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss= d_loss)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "# generator = pickle.load(open('generator3.p','rb'))\n",
    "# discriminator = pickle.load(open('discriminator3.p','rb'))\n",
    "'''\n",
    "The adversarial network:對抗網路\n",
    "最後，設定GAN，它連結生成器（generator）和鑑別器（discrimitor）。 這是一種模型，經過訓練，\n",
    "將使生成器（generator）朝著提高其愚弄鑑別器（discrimitor）能力的方向移動。 該模型將潛在的空間點轉換為分類決策，\n",
    "“假的”或“真實的”，並且意味著使用始終是“這些是真實影象”的標籤來訓練。 所以訓練`gan`將以一種方式更新\n",
    "“發生器”的權重，使得“鑑別器”在檢視假影象時更可能預測“真實”。 非常重要的是，將鑑別器設定為在訓練\n",
    "期間被凍結（不可訓練）：訓練“gan”時其權重不會更新。 如果在此過程中可以更新鑑別器權重，那麼將訓練鑑別\n",
    "器始終預測“真實”。\n",
    "'''\n",
    "# 將鑑別器（discrimitor）權重設定為不可訓練（僅適用於`gan`模型）\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "#losses = [ mutual_info_loss]\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0005)\n",
    "gan.compile(optimizer=gan_optimizer, loss= d_loss)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# gan = pickle.load(open('gan.p','rb'))\n",
    "'''\n",
    "  開始訓練了。\n",
    "  每個epoch：\n",
    "   *在潛在空間中繪製隨機點（隨機噪聲）。\n",
    "   *使用此隨機噪聲生成帶有“generator”的影象。\n",
    "   *將生成的影象與實際影象混合。\n",
    "   *使用這些混合影象訓練“鑑別器”，使用相應的目標，“真實”（對於真實影象）或“假”（對於生成的影象）。\n",
    "   *在潛在空間中繪製新的隨機點。\n",
    "   *使用這些隨機向量訓練“gan”，目標都是“這些是真實的影象”。 這將更新發生器的權重（僅因為鑑別器在“gan”內被凍結）\n",
    "   以使它們朝向獲得鑑別器以預測所生成影象的“這些是真實影象”，即這訓練發生器欺騙鑑別器。\n",
    "'''\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "'''\n",
    "# 匯入CIFAR10資料集\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# 從CIFAR10資料集中選擇frog類（class 6）\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "# 標準化資料\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "'''\n",
    "\n",
    "dirPath = 'C:/Users/User/GAN/SA'\n",
    "SA = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "x_train = []\n",
    "for img in SA:\n",
    "    imgcv = cv2.imread(dirPath+'/'+img)\n",
    "    imgcv = cv2.cvtColor(imgcv, cv2.COLOR_BGR2RGB)\n",
    "    x_train.append(imgcv)\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "\n",
    "iterations = 450000000000\n",
    "batch_size = 20\n",
    "save_dir = '.\\\\Wgan_image'\n",
    "\n",
    "start = 0 \n",
    "disIteration = 5\n",
    "    \n",
    "# 開始訓練迭代\n",
    "for step in range(iterations):\n",
    "    # 在潛在空間中抽樣隨機點\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    # 將隨機抽樣點解碼為假影象\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    # 將假影象與真實影象進行比較\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    \n",
    "    # 組裝區別真假影象的標籤\n",
    "    # 在WGAN的時候，將真實圖片標為負一，生成圖片為正一\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                            -np.ones((batch_size, 1))])\n",
    "    # 重要的技巧，在標籤上新增隨機噪聲\n",
    "    labels += 0.1 * np.random.random(labels.shape)\n",
    "    labels -= 0.1 * np.random.random(labels.shape)\n",
    "    # 訓練鑑別器（discrimitor）\n",
    "    for i in range(disIteration):\n",
    "        d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    # 在潛在空間中取樣隨機點\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    # 彙集標有“所有真實影象”的標籤\n",
    "    misleading_targets = -np.ones((batch_size, 1))\n",
    "    \n",
    "    # 訓練生成器（generator）（通過gan模型，鑑別器（discrimitor）權值被凍結）\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    if step % 100 == 0:\n",
    "        # 儲存網路權值\n",
    "        #gan.save_weights('gan.h5')\n",
    "        if step % 100 == 0:\n",
    "            print(discriminator.predict(combined_images))\n",
    "        # 輸出metrics\n",
    "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
    "        print('adversarial loss at step %s: %s' % (step, a_loss))\n",
    "\n",
    "        # 儲存生成的影象\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, str(step) + '.png'))\n",
    "        \n",
    "        pickle.dump(discriminator,open('discriminatorSinWGAN.p','wb'))\n",
    "        pickle.dump(generator,open('generatorSinWGAN.p','wb'))\n",
    "        pickle.dump(gan,open('ganSinWGAN.p','wb'))\n",
    "        \n",
    "        # 儲存真實影象，以便進行比較\n",
    "#         img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "#         img.save(os.path.join(save_dir, 'real_SA' + str(step) + '.png'))\n",
    "    if step % 1000 == 0:\n",
    "        clear_output()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# 繪圖\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 在潛在空間中抽樣隨機點\n",
    "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
    "\n",
    "# 將隨機抽樣點解碼為假影象\n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "for i in range(generated_images.shape[0]):\n",
    "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-8.5614627e+18],\n",
      "       [-3.1325375e+20],\n",
      "       [-8.5618382e+18],\n",
      "       [-8.5618421e+18],\n",
      "       [-4.2427409e+19],\n",
      "       [ 3.5217530e+20],\n",
      "       [-8.5618421e+18],\n",
      "       [-4.4988656e+20],\n",
      "       [ 5.6595685e+20],\n",
      "       [ 2.5334111e+20],\n",
      "       [-2.0015350e+20],\n",
      "       [-6.8101771e+18],\n",
      "       [ 6.3091035e+17],\n",
      "       [-8.5500592e+18],\n",
      "       [-3.0570247e+19],\n",
      "       [-8.5613847e+18],\n",
      "       [-8.5777531e+18],\n",
      "       [ 4.1935475e+19],\n",
      "       [-8.5618421e+18],\n",
      "       [-8.5618421e+18],\n",
      "       [ 5.7457367e+23],\n",
      "       [ 7.3118837e+23],\n",
      "       [ 5.9840881e+23],\n",
      "       [ 7.2080451e+23],\n",
      "       [ 6.7553836e+23],\n",
      "       [ 6.7417892e+23],\n",
      "       [ 5.9610693e+23],\n",
      "       [ 6.7353343e+23],\n",
      "       [ 3.3557340e+23],\n",
      "       [ 5.4720937e+23],\n",
      "       [ 3.8685780e+23],\n",
      "       [ 6.7605833e+23],\n",
      "       [ 6.3449356e+23],\n",
      "       [ 3.5025564e+23],\n",
      "       [ 6.5580640e+23],\n",
      "       [ 5.1896268e+23],\n",
      "       [ 5.4621566e+23],\n",
      "       [ 5.5122301e+23],\n",
      "       [ 5.7653177e+23],\n",
      "       [ 6.7568449e+23]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\n",
      "discriminator loss at step 7100: [-2.9033613e+23, -2.9045926e+23, 1.23131255e+20]\n",
      "adversarial loss at step 7100: [7.535449e+17, 0.0, 7.535449e+17]\n",
      "[array([[2.8503671e+19],\n",
      "       [3.0684058e+19],\n",
      "       [2.8503671e+19],\n",
      "       [3.4785707e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8995309e+19],\n",
      "       [1.0236494e+20],\n",
      "       [1.2111821e+20],\n",
      "       [2.8504923e+19],\n",
      "       [8.9373002e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8503671e+19],\n",
      "       [9.0686576e+19],\n",
      "       [2.8503671e+19],\n",
      "       [2.8503443e+19],\n",
      "       [2.8503671e+19],\n",
      "       [5.9418490e+23],\n",
      "       [6.6067129e+23],\n",
      "       [5.8303003e+23],\n",
      "       [5.5119145e+23],\n",
      "       [7.5842866e+23],\n",
      "       [7.0696585e+23],\n",
      "       [5.8779765e+23],\n",
      "       [6.6067129e+23],\n",
      "       [5.5603289e+23],\n",
      "       [5.9045704e+23],\n",
      "       [5.5119145e+23],\n",
      "       [5.8779765e+23],\n",
      "       [6.8990160e+23],\n",
      "       [5.8799887e+23],\n",
      "       [6.1845361e+23],\n",
      "       [5.9505428e+23],\n",
      "       [3.9737227e+23],\n",
      "       [7.4510420e+23],\n",
      "       [6.8320356e+23],\n",
      "       [3.6168621e+23]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\n",
      "discriminator loss at step 7200: [-2.9875642e+23, -2.9890782e+23, 1.5139103e+20]\n",
      "adversarial loss at step 7200: [4.538027e+17, 0.0, 4.538027e+17]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f272e4296cd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mrandom_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;31m# 將隨機抽樣點解碼為假影象\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0mgenerated_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_latent_vectors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;31m# 將假影象與真實影象進行比較\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "生成器（generator）\n",
    "首先，建立一個“生成器（generator）”模型，它將一個向量（從潛在空間 - 在訓練期間隨機取樣）轉換為候選影象。\n",
    "GAN通常出現的許多問題之一是generator卡在生成的影象上，看起來像噪聲。一種可能的解決方案是在鑑別器（discriminator）\n",
    "和生成器（generator）上使用dropout。\n",
    "'''\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "num_classes = 52\n",
    "latent_dim = 46\n",
    "height = 92\n",
    "width = 68\n",
    "channels = 3\n",
    "\n",
    "# 兩個input，分別是亂數、類別\n",
    "latent = keras.Input(shape=(latent_dim,))\n",
    "image_class = Input(shape=(1,), dtype='int32')\n",
    "cls = Embedding(num_classes, latent_dim,\n",
    "                embeddings_initializer='glorot_normal')(image_class)\n",
    "\n",
    "generator_input = layers.add([latent, cls])\n",
    "\n",
    "x = layers.Dense(128 * 46 * 34)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Reshape((46, 34, 128))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, 4, strides=2, padding='same',kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same',kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# 生成一個 3 channel 的feature map\n",
    "x = layers.Conv2D(channels, 16, activation='tanh', padding='same',kernel_initializer=weight_init)(x)\n",
    "generator = keras.models.Model([latent, image_class], x)\n",
    "generator.summary()\n",
    "\n",
    "'''\n",
    "discriminator(鑑別器)\n",
    "建立鑑別器模型，它將候選影象（真實的或合成的）作為輸入，並將其分為兩類：“生成的影象”或“來自訓練集的真實影象”。\n",
    "'''\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3, kernel_initializer=weight_init)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(64, 4, strides=2, kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Conv2D(32, 4, strides=2, kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(32)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Conv2D(32, 4, strides=2, kernel_initializer=weight_init)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# 重要的技巧（新增一個dropout層）\n",
    "x = layers.Dropout(0,4)(x)\n",
    "\n",
    "# 分類層\n",
    "#isFake = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "isFake = layers.Dense(1)(x)\n",
    "classDetac = layers.Dense(num_classes, activation ='softmax')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, [isFake,classDetac])\n",
    "discriminator.summary()\n",
    "\n",
    "# In[11]:\n",
    "# generator = pickle.load(open('generatorWGAN.p','rb'))\n",
    "# discriminator = pickle.load(open('discriminatorWGAN.p','rb'))\n",
    "# 為了訓練穩定，在優化器中使用學習率衰減和梯度限幅（按值）。\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0005)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss=[d_loss, 'sparse_categorical_crossentropy'])\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "'''\n",
    "The adversarial network:對抗網路\n",
    "最後，設定GAN，它連結生成器（generator）和鑑別器（discrimitor）。 這是一種模型，經過訓練，\n",
    "將使生成器（generator）朝著提高其愚弄鑑別器（discrimitor）能力的方向移動。 該模型將潛在的空間點轉換為分類決策，\n",
    "“假的”或“真實的”，並且意味著使用始終是“這些是真實影象”的標籤來訓練。 所以訓練`gan`將以一種方式更新\n",
    "“發生器”的權重，使得“鑑別器”在檢視假影象時更可能預測“真實”。 非常重要的是，將鑑別器設定為在訓練\n",
    "期間被凍結（不可訓練）：訓練“gan”時其權重不會更新。 如果在此過程中可以更新鑑別器權重，那麼將訓練鑑別\n",
    "器始終預測“真實”。\n",
    "'''\n",
    "# 將鑑別器（discrimitor）權重設定為不可訓練（僅適用於`gan`模型）\n",
    "\n",
    "#discriminator.trainable = False\n",
    "\n",
    "fakeImg = generator([latent, image_class])\n",
    "fake, aux = discriminator(fakeImg)\n",
    "\n",
    "gan = keras.models.Model([latent, image_class], [fake, aux])\n",
    "#losses = [ mutual_info_loss]\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0005)\n",
    "gan.compile(optimizer=gan_optimizer, loss=[d_loss, 'sparse_categorical_crossentropy'])\n",
    "gan.summary()\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# gan = pickle.load(open('gan.p','rb'))\n",
    "'''\n",
    "  開始訓練了。\n",
    "  每個epoch：\n",
    "   *在潛在空間中繪製隨機點（隨機噪聲）。\n",
    "   *使用此隨機噪聲生成帶有“generator”的影象。\n",
    "   *將生成的影象與實際影象混合。\n",
    "   *使用這些混合影象訓練“鑑別器”，使用相應的目標，“真實”（對於真實影象）或“假”（對於生成的影象）。\n",
    "   *在潛在空間中繪製新的隨機點。\n",
    "   *使用這些隨機向量訓練“gan”，目標都是“這些是真實的影象”。 這將更新發生器的權重（僅因為鑑別器在“gan”內被凍結）\n",
    "   以使它們朝向獲得鑑別器以預測所生成影象的“這些是真實影象”，即這訓練發生器欺騙鑑別器。\n",
    " '''\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "'''\n",
    "# 匯入CIFAR10資料集\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# 從CIFAR10資料集中選擇frog類（class 6）\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "# 標準化資料\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "'''\n",
    "\n",
    "dirPath = 'C:/Users/User/GAN/SA'\n",
    "SA = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "x_train = []\n",
    "for img in SA:\n",
    "    imgcv = cv2.imread(dirPath+'/'+img)\n",
    "    imgcv = cv2.cvtColor(imgcv, cv2.COLOR_BGR2RGB)\n",
    "    x_train.append(imgcv)\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "lableImg = pickle.load(open('lableImg.p','rb'))\n",
    "\n",
    "iterations = 450000000000\n",
    "batch_size = 20\n",
    "save_dir = '.\\\\Wgan_image'\n",
    "\n",
    "start = 0 \n",
    "disIteration = 5\n",
    "# 開始訓練迭代\n",
    "for step in range(iterations):\n",
    "    # 在潛在空間中抽樣隨機點\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    # 隨機選擇化哪個類別\n",
    "    random_labels = np.random.randint(0, num_classes, batch_size)\n",
    "    # 將隨機抽樣點解碼為假影象\n",
    "    generated_images = generator.predict([random_latent_vectors,random_labels.reshape((-1, 1))])\n",
    "    \n",
    "    # 將假影象與真實影象進行比較\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    \n",
    "    # 取真實類別的標籤\n",
    "    \n",
    "    \n",
    "    # 組裝區別真假影象的標籤\n",
    "    # 在WGAN的時候，將真實圖片標為負一，生成圖片為正一\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                            -np.ones((batch_size, 1))])\n",
    "    # 組裝區別真假類別的標籤\n",
    "    claLabels = lableImg[start: stop]\n",
    "    combined_clsLable = np.concatenate([random_labels, claLabels])\n",
    "    #combined_clsLable = np.concatenate([np.negative(np.ones((batch_size))), claLabels])\n",
    "    \n",
    "    # 重要的技巧，在標籤上新增隨機噪聲\n",
    "    labels += 0.1 * np.random.random(labels.shape)\n",
    "    labels -= 0.1 * np.random.random(labels.shape)\n",
    "    # Assign sample weight\n",
    "    disc_sample_weight = [np.ones(2 * (batch_size)),\n",
    "                                  np.concatenate((np.zeros(batch_size) ,\n",
    "                                                  np.ones(batch_size)*3))]\n",
    "    # 訓練鑑別器（discrimitor）\n",
    "    for i in range(disIteration):\n",
    "        d_loss = discriminator.train_on_batch(combined_images, [labels,combined_clsLable], sample_weight=disc_sample_weight)\n",
    "    # 當生成器訓練超過5000次，判斷器改練5次\n",
    "    if step > 5000:\n",
    "        disIteration = 2\n",
    "\n",
    "    \n",
    "    # 彙集標有“所有真實影象”的標籤\n",
    "    misleading_targets = -np.ones((batch_size, 1))\n",
    "    \n",
    "    # 再做一個新的亂數和標籤\n",
    "    # 在潛在空間中抽樣隨機點\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    # 隨機選擇化哪個類別\n",
    "    random_labels = np.random.randint(0, num_classes, batch_size)\n",
    "    \n",
    "    # 訓練生成器（generator）（通過gan模型，鑑別器（discrimitor）權值被凍結）\n",
    "    a_loss = gan.train_on_batch([random_latent_vectors,random_labels.reshape((-1, 1))], [misleading_targets,random_labels])\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    if step % 100 == 0:\n",
    "        # 儲存網路權值\n",
    "        gan.save_weights('gan.h5')\n",
    "        if step % 100 == 0:\n",
    "            print(discriminator.predict(combined_images))\n",
    "        # 輸出metrics\n",
    "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
    "        print('adversarial loss at step %s: %s' % (step, a_loss))\n",
    "\n",
    "        # 儲存生成的影象\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, str(step) + '.png'))\n",
    "        \n",
    "        pickle.dump(discriminator,open('discriminatorWGAN.p','wb'))\n",
    "        pickle.dump(generator,open('generatorWGAN.p','wb'))\n",
    "        pickle.dump(gan,open('ganWGAN.p','wb'))\n",
    "        \n",
    "        # 儲存真實影象，以便進行比較\n",
    "#         img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "#         img.save(os.path.join(save_dir, 'real_SA' + str(step) + '.png'))\n",
    "    if step % 1000 == 0:\n",
    "        clear_output()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# 繪圖\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 在潛在空間中抽樣隨機點\n",
    "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
    "\n",
    "# 將隨機抽樣點解碼為假影象\n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "for i in range(generated_images.shape[0]):\n",
    "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_size = 20\n",
    "labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                            -np.ones((batch_size, 1))])\n",
    "labels += 0.1 * np.random.random(labels.shape)\n",
    "labels -= 0.1 * np.random.random(labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example of a wgan for generating handwritten digits\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras import backend\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(Constraint):\n",
    "\t# set clip value when initialized\n",
    "\tdef __init__(self, clip_value):\n",
    "\t\tself.clip_value = clip_value\n",
    "\n",
    "\t# clip model weights to hypercube\n",
    "\tdef __call__(self, weights):\n",
    "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "\t# get the config\n",
    "\tdef get_config(self):\n",
    "\t\treturn {'clip_value': self.clip_value}\n",
    "\n",
    "# calculate wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "\treturn backend.mean(y_true * y_pred)\n",
    "\n",
    "# define the standalone critic model\n",
    "def define_critic(in_shape=(28,28,1)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = ClipConstraint(0.01)\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\t# downsample to 14x14\n",
    "\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# downsample to 7x7\n",
    "\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# scoring, linear activation\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1))\n",
    "\t# compile model\n",
    "\topt = RMSprop(lr=0.00005)\n",
    "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\t# foundation for 7x7 image\n",
    "\tn_nodes = 128 * 7 * 7\n",
    "\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Reshape((7, 7, 128)))\n",
    "\t# upsample to 14x14\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# upsample to 28x28\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# output 28x28x1\n",
    "\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and critic model, for updating the generator\n",
    "def define_gan(generator, critic):\n",
    "\t# make weights in the critic not trainable\n",
    "\tcritic.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the critic\n",
    "\tmodel.add(critic)\n",
    "\t# compile model\n",
    "\topt = RMSprop(lr=0.00005)\n",
    "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "\treturn model\n",
    "\n",
    "# load images\n",
    "def load_real_samples():\n",
    "\t# load dataset\n",
    "\t(trainX, trainy), (_, _) = load_data()\n",
    "\t# select all of the examples for a given class\n",
    "\tselected_ix = trainy == 7\n",
    "\tX = trainX[selected_ix]\n",
    "\t# expand to 3d, e.g. add channels\n",
    "\tX = expand_dims(X, axis=-1)\n",
    "\t# convert from ints to floats\n",
    "\tX = X.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX = (X - 127.5) / 127.5\n",
    "\treturn X\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# select images\n",
    "\tX = dataset[ix]\n",
    "\t# generate class labels, -1 for 'real'\n",
    "\ty = -ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels with 1.0 for 'fake'\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "\t# prepare fake examples\n",
    "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\tX = (X + 1) / 2.0\n",
    "\t# plot images\n",
    "\tfor i in range(10 * 10):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(10, 10, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%04d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "\t# plot history\n",
    "\tpyplot.plot(d1_hist, label='crit_real')\n",
    "\tpyplot.plot(d2_hist, label='crit_fake')\n",
    "\tpyplot.plot(g_hist, label='gen')\n",
    "\tpyplot.legend()\n",
    "\tpyplot.savefig('plot_line_plot_loss.png')\n",
    "\tpyplot.close()\n",
    "\n",
    "# train the generator and critic\n",
    "def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# calculate the size of half a batch of samples\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# lists for keeping track of loss\n",
    "\tc1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# update the critic more than the generator\n",
    "\t\tc1_tmp, c2_tmp = list(), list()\n",
    "\t\tfor _ in range(n_critic):\n",
    "\t\t\t# get randomly selected 'real' samples\n",
    "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t\t# update critic model weights\n",
    "\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
    "\t\t\tc1_tmp.append(c_loss1)\n",
    "\t\t\t# generate 'fake' examples\n",
    "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t\t# update critic model weights\n",
    "\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t\tc2_tmp.append(c_loss2)\n",
    "\t\t# store critic loss\n",
    "\t\tc1_hist.append(mean(c1_tmp))\n",
    "\t\tc2_hist.append(mean(c2_tmp))\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = -ones((n_batch, 1))\n",
    "\t\t# update the generator via the critic's error\n",
    "\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\t\tg_hist.append(g_loss)\n",
    "\t\t# summarize loss on this batch\n",
    "\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "\t\t# evaluate the model performance every 'epoch'\n",
    "\t\tif (i+1) % bat_per_epo == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, latent_dim)\n",
    "\t# line plots of loss\n",
    "\tplot_history(c1_hist, c2_hist, g_hist)\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 50\n",
    "# create the critic\n",
    "critic = define_critic()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, critic)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)\n",
    "# train model\n",
    "train(generator, critic, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-worker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker CollectiveAllReduceStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CollectiveCommunication.AUTO\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
